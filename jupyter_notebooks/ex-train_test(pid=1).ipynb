{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c438f4-4a6c-48f1-be15-b60ad25888ca",
   "metadata": {},
   "source": [
    "# ex-train_test(pid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bdaa0e0-3004-4eaa-9877-bbca287eccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/SentenceAx\n"
     ]
    }
   ],
   "source": [
    "# this makes sure it starts looking for things from the scumpy folder down.\n",
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.insert(0,os.getcwd())\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f9fa23-d7c2-4c5a-a16f-38dfd02c2f89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(os.environ[\"TOKENIZERS_PARALLELISM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf61af3b-4c82-4860-9641-e631e5107e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Params import *\n",
    "from MConductor import *\n",
    "\n",
    "\n",
    "def main(pid):\n",
    "    params = Params(pid)\n",
    "    params.d[\"refresh_cache\"] = True\n",
    "    params.d[\"gpus\"] = 0\n",
    "    conductor = MConductor(params, verbose_model= True)\n",
    "    conductor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2a9f60-9297-4129-8dd0-edc71196421d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** new params\n",
      "new params: pid=1, task='ex', mode='train_test'\n",
      "params=\n",
      "{'accumulate_grad_batches': 1,\n",
      " 'batch_size': 24,\n",
      " 'con_weight_str': '1',\n",
      " 'do_rescoring': False,\n",
      " 'dropout_fun': 0.0,\n",
      " 'epochs': 30,\n",
      " 'gpus': 1,\n",
      " 'gradient_clip_val': 5,\n",
      " 'iterative_layers': 2,\n",
      " 'lr': 2e-05,\n",
      " 'mode': 'train_test',\n",
      " 'model_str': 'bert-base-cased',\n",
      " 'num_extractions': 5,\n",
      " 'optimizer': 'adamW',\n",
      " 'refresh_cache': False,\n",
      " 'save_k': 1,\n",
      " 'suggested_checkpoint_fp': '',\n",
      " 'task': 'ex',\n",
      " 'val_check_interval': 1.0,\n",
      " 'wreg': 0,\n",
      " 'write_allen_file': True,\n",
      " 'write_extags_file': True}\n",
      "\n",
      "MInput started reading 'input_data/carb-data/test.txt'\n",
      "\n",
      "MInput finished reading 'input_data/carb-data/test.txt'\n",
      "number of lines= 1283\n",
      "number of used samples=  642\n",
      "number of omitted samples=  0\n",
      "\n",
      "MInput started reading 'input_data/carb-data/dev.txt'\n",
      "\n",
      "MInput finished reading 'input_data/carb-data/dev.txt'\n",
      "number of lines= 1283\n",
      "number of used samples=  642\n",
      "number of omitted samples=  0\n",
      "\n",
      "MInput started reading 'input_data/carb-data/test.txt'\n",
      "\n",
      "MInput finished reading 'input_data/carb-data/test.txt'\n",
      "number of lines= 1283\n",
      "number of used samples=  642\n",
      "number of omitted samples=  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "INFO:lightning:GPU available: False, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "WARNING:lightning:No environment variable for node rank defined. Set as 0.\n",
      "\n",
      "    | Name                                                        | Type              | Params\n",
      "----------------------------------------------------------------------------------------------\n",
      "0   | base_model                                                  | BertModel         | 94 M  \n",
      "1   | base_model.embeddings                                       | BertEmbeddings    | 22 M  \n",
      "2   | base_model.embeddings.word_embeddings                       | Embedding         | 22 M  \n",
      "3   | base_model.embeddings.position_embeddings                   | Embedding         | 393 K \n",
      "4   | base_model.embeddings.token_type_embeddings                 | Embedding         | 1 K   \n",
      "5   | base_model.embeddings.LayerNorm                             | LayerNorm         | 1 K   \n",
      "6   | base_model.embeddings.dropout                               | Dropout           | 0     \n",
      "7   | base_model.encoder                                          | BertEncoder       | 70 M  \n",
      "8   | base_model.encoder.layer                                    | ModuleList        | 70 M  \n",
      "9   | base_model.encoder.layer.0                                  | BertLayer         | 7 M   \n",
      "10  | base_model.encoder.layer.0.attention                        | BertAttention     | 2 M   \n",
      "11  | base_model.encoder.layer.0.attention.self                   | BertSelfAttention | 1 M   \n",
      "12  | base_model.encoder.layer.0.attention.self.query             | Linear            | 590 K \n",
      "13  | base_model.encoder.layer.0.attention.self.key               | Linear            | 590 K \n",
      "14  | base_model.encoder.layer.0.attention.self.value             | Linear            | 590 K \n",
      "15  | base_model.encoder.layer.0.attention.self.dropout           | Dropout           | 0     \n",
      "16  | base_model.encoder.layer.0.attention.output                 | BertSelfOutput    | 592 K \n",
      "17  | base_model.encoder.layer.0.attention.output.dense           | Linear            | 590 K \n",
      "18  | base_model.encoder.layer.0.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "19  | base_model.encoder.layer.0.attention.output.dropout         | Dropout           | 0     \n",
      "20  | base_model.encoder.layer.0.intermediate                     | BertIntermediate  | 2 M   \n",
      "21  | base_model.encoder.layer.0.intermediate.dense               | Linear            | 2 M   \n",
      "22  | base_model.encoder.layer.0.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "23  | base_model.encoder.layer.0.output                           | BertOutput        | 2 M   \n",
      "24  | base_model.encoder.layer.0.output.dense                     | Linear            | 2 M   \n",
      "25  | base_model.encoder.layer.0.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "26  | base_model.encoder.layer.0.output.dropout                   | Dropout           | 0     \n",
      "27  | base_model.encoder.layer.1                                  | BertLayer         | 7 M   \n",
      "28  | base_model.encoder.layer.1.attention                        | BertAttention     | 2 M   \n",
      "29  | base_model.encoder.layer.1.attention.self                   | BertSelfAttention | 1 M   \n",
      "30  | base_model.encoder.layer.1.attention.self.query             | Linear            | 590 K \n",
      "31  | base_model.encoder.layer.1.attention.self.key               | Linear            | 590 K \n",
      "32  | base_model.encoder.layer.1.attention.self.value             | Linear            | 590 K \n",
      "33  | base_model.encoder.layer.1.attention.self.dropout           | Dropout           | 0     \n",
      "34  | base_model.encoder.layer.1.attention.output                 | BertSelfOutput    | 592 K \n",
      "35  | base_model.encoder.layer.1.attention.output.dense           | Linear            | 590 K \n",
      "36  | base_model.encoder.layer.1.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "37  | base_model.encoder.layer.1.attention.output.dropout         | Dropout           | 0     \n",
      "38  | base_model.encoder.layer.1.intermediate                     | BertIntermediate  | 2 M   \n",
      "39  | base_model.encoder.layer.1.intermediate.dense               | Linear            | 2 M   \n",
      "40  | base_model.encoder.layer.1.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "41  | base_model.encoder.layer.1.output                           | BertOutput        | 2 M   \n",
      "42  | base_model.encoder.layer.1.output.dense                     | Linear            | 2 M   \n",
      "43  | base_model.encoder.layer.1.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "44  | base_model.encoder.layer.1.output.dropout                   | Dropout           | 0     \n",
      "45  | base_model.encoder.layer.2                                  | BertLayer         | 7 M   \n",
      "46  | base_model.encoder.layer.2.attention                        | BertAttention     | 2 M   \n",
      "47  | base_model.encoder.layer.2.attention.self                   | BertSelfAttention | 1 M   \n",
      "48  | base_model.encoder.layer.2.attention.self.query             | Linear            | 590 K \n",
      "49  | base_model.encoder.layer.2.attention.self.key               | Linear            | 590 K \n",
      "50  | base_model.encoder.layer.2.attention.self.value             | Linear            | 590 K \n",
      "51  | base_model.encoder.layer.2.attention.self.dropout           | Dropout           | 0     \n",
      "52  | base_model.encoder.layer.2.attention.output                 | BertSelfOutput    | 592 K \n",
      "53  | base_model.encoder.layer.2.attention.output.dense           | Linear            | 590 K \n",
      "54  | base_model.encoder.layer.2.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "55  | base_model.encoder.layer.2.attention.output.dropout         | Dropout           | 0     \n",
      "56  | base_model.encoder.layer.2.intermediate                     | BertIntermediate  | 2 M   \n",
      "57  | base_model.encoder.layer.2.intermediate.dense               | Linear            | 2 M   \n",
      "58  | base_model.encoder.layer.2.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "59  | base_model.encoder.layer.2.output                           | BertOutput        | 2 M   \n",
      "60  | base_model.encoder.layer.2.output.dense                     | Linear            | 2 M   \n",
      "61  | base_model.encoder.layer.2.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "62  | base_model.encoder.layer.2.output.dropout                   | Dropout           | 0     \n",
      "63  | base_model.encoder.layer.3                                  | BertLayer         | 7 M   \n",
      "64  | base_model.encoder.layer.3.attention                        | BertAttention     | 2 M   \n",
      "65  | base_model.encoder.layer.3.attention.self                   | BertSelfAttention | 1 M   \n",
      "66  | base_model.encoder.layer.3.attention.self.query             | Linear            | 590 K \n",
      "67  | base_model.encoder.layer.3.attention.self.key               | Linear            | 590 K \n",
      "68  | base_model.encoder.layer.3.attention.self.value             | Linear            | 590 K \n",
      "69  | base_model.encoder.layer.3.attention.self.dropout           | Dropout           | 0     \n",
      "70  | base_model.encoder.layer.3.attention.output                 | BertSelfOutput    | 592 K \n",
      "71  | base_model.encoder.layer.3.attention.output.dense           | Linear            | 590 K \n",
      "72  | base_model.encoder.layer.3.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "73  | base_model.encoder.layer.3.attention.output.dropout         | Dropout           | 0     \n",
      "74  | base_model.encoder.layer.3.intermediate                     | BertIntermediate  | 2 M   \n",
      "75  | base_model.encoder.layer.3.intermediate.dense               | Linear            | 2 M   \n",
      "76  | base_model.encoder.layer.3.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "77  | base_model.encoder.layer.3.output                           | BertOutput        | 2 M   \n",
      "78  | base_model.encoder.layer.3.output.dense                     | Linear            | 2 M   \n",
      "79  | base_model.encoder.layer.3.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "80  | base_model.encoder.layer.3.output.dropout                   | Dropout           | 0     \n",
      "81  | base_model.encoder.layer.4                                  | BertLayer         | 7 M   \n",
      "82  | base_model.encoder.layer.4.attention                        | BertAttention     | 2 M   \n",
      "83  | base_model.encoder.layer.4.attention.self                   | BertSelfAttention | 1 M   \n",
      "84  | base_model.encoder.layer.4.attention.self.query             | Linear            | 590 K \n",
      "85  | base_model.encoder.layer.4.attention.self.key               | Linear            | 590 K \n",
      "86  | base_model.encoder.layer.4.attention.self.value             | Linear            | 590 K \n",
      "87  | base_model.encoder.layer.4.attention.self.dropout           | Dropout           | 0     \n",
      "88  | base_model.encoder.layer.4.attention.output                 | BertSelfOutput    | 592 K \n",
      "89  | base_model.encoder.layer.4.attention.output.dense           | Linear            | 590 K \n",
      "90  | base_model.encoder.layer.4.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "91  | base_model.encoder.layer.4.attention.output.dropout         | Dropout           | 0     \n",
      "92  | base_model.encoder.layer.4.intermediate                     | BertIntermediate  | 2 M   \n",
      "93  | base_model.encoder.layer.4.intermediate.dense               | Linear            | 2 M   \n",
      "94  | base_model.encoder.layer.4.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "95  | base_model.encoder.layer.4.output                           | BertOutput        | 2 M   \n",
      "96  | base_model.encoder.layer.4.output.dense                     | Linear            | 2 M   \n",
      "97  | base_model.encoder.layer.4.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "98  | base_model.encoder.layer.4.output.dropout                   | Dropout           | 0     \n",
      "99  | base_model.encoder.layer.5                                  | BertLayer         | 7 M   \n",
      "100 | base_model.encoder.layer.5.attention                        | BertAttention     | 2 M   \n",
      "101 | base_model.encoder.layer.5.attention.self                   | BertSelfAttention | 1 M   \n",
      "102 | base_model.encoder.layer.5.attention.self.query             | Linear            | 590 K \n",
      "103 | base_model.encoder.layer.5.attention.self.key               | Linear            | 590 K \n",
      "104 | base_model.encoder.layer.5.attention.self.value             | Linear            | 590 K \n",
      "105 | base_model.encoder.layer.5.attention.self.dropout           | Dropout           | 0     \n",
      "106 | base_model.encoder.layer.5.attention.output                 | BertSelfOutput    | 592 K \n",
      "107 | base_model.encoder.layer.5.attention.output.dense           | Linear            | 590 K \n",
      "108 | base_model.encoder.layer.5.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "109 | base_model.encoder.layer.5.attention.output.dropout         | Dropout           | 0     \n",
      "110 | base_model.encoder.layer.5.intermediate                     | BertIntermediate  | 2 M   \n",
      "111 | base_model.encoder.layer.5.intermediate.dense               | Linear            | 2 M   \n",
      "112 | base_model.encoder.layer.5.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "113 | base_model.encoder.layer.5.output                           | BertOutput        | 2 M   \n",
      "114 | base_model.encoder.layer.5.output.dense                     | Linear            | 2 M   \n",
      "115 | base_model.encoder.layer.5.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "116 | base_model.encoder.layer.5.output.dropout                   | Dropout           | 0     \n",
      "117 | base_model.encoder.layer.6                                  | BertLayer         | 7 M   \n",
      "118 | base_model.encoder.layer.6.attention                        | BertAttention     | 2 M   \n",
      "119 | base_model.encoder.layer.6.attention.self                   | BertSelfAttention | 1 M   \n",
      "120 | base_model.encoder.layer.6.attention.self.query             | Linear            | 590 K \n",
      "121 | base_model.encoder.layer.6.attention.self.key               | Linear            | 590 K \n",
      "122 | base_model.encoder.layer.6.attention.self.value             | Linear            | 590 K \n",
      "123 | base_model.encoder.layer.6.attention.self.dropout           | Dropout           | 0     \n",
      "124 | base_model.encoder.layer.6.attention.output                 | BertSelfOutput    | 592 K \n",
      "125 | base_model.encoder.layer.6.attention.output.dense           | Linear            | 590 K \n",
      "126 | base_model.encoder.layer.6.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "127 | base_model.encoder.layer.6.attention.output.dropout         | Dropout           | 0     \n",
      "128 | base_model.encoder.layer.6.intermediate                     | BertIntermediate  | 2 M   \n",
      "129 | base_model.encoder.layer.6.intermediate.dense               | Linear            | 2 M   \n",
      "130 | base_model.encoder.layer.6.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "131 | base_model.encoder.layer.6.output                           | BertOutput        | 2 M   \n",
      "132 | base_model.encoder.layer.6.output.dense                     | Linear            | 2 M   \n",
      "133 | base_model.encoder.layer.6.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "134 | base_model.encoder.layer.6.output.dropout                   | Dropout           | 0     \n",
      "135 | base_model.encoder.layer.7                                  | BertLayer         | 7 M   \n",
      "136 | base_model.encoder.layer.7.attention                        | BertAttention     | 2 M   \n",
      "137 | base_model.encoder.layer.7.attention.self                   | BertSelfAttention | 1 M   \n",
      "138 | base_model.encoder.layer.7.attention.self.query             | Linear            | 590 K \n",
      "139 | base_model.encoder.layer.7.attention.self.key               | Linear            | 590 K \n",
      "140 | base_model.encoder.layer.7.attention.self.value             | Linear            | 590 K \n",
      "141 | base_model.encoder.layer.7.attention.self.dropout           | Dropout           | 0     \n",
      "142 | base_model.encoder.layer.7.attention.output                 | BertSelfOutput    | 592 K \n",
      "143 | base_model.encoder.layer.7.attention.output.dense           | Linear            | 590 K \n",
      "144 | base_model.encoder.layer.7.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "145 | base_model.encoder.layer.7.attention.output.dropout         | Dropout           | 0     \n",
      "146 | base_model.encoder.layer.7.intermediate                     | BertIntermediate  | 2 M   \n",
      "147 | base_model.encoder.layer.7.intermediate.dense               | Linear            | 2 M   \n",
      "148 | base_model.encoder.layer.7.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "149 | base_model.encoder.layer.7.output                           | BertOutput        | 2 M   \n",
      "150 | base_model.encoder.layer.7.output.dense                     | Linear            | 2 M   \n",
      "151 | base_model.encoder.layer.7.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "152 | base_model.encoder.layer.7.output.dropout                   | Dropout           | 0     \n",
      "153 | base_model.encoder.layer.8                                  | BertLayer         | 7 M   \n",
      "154 | base_model.encoder.layer.8.attention                        | BertAttention     | 2 M   \n",
      "155 | base_model.encoder.layer.8.attention.self                   | BertSelfAttention | 1 M   \n",
      "156 | base_model.encoder.layer.8.attention.self.query             | Linear            | 590 K \n",
      "157 | base_model.encoder.layer.8.attention.self.key               | Linear            | 590 K \n",
      "158 | base_model.encoder.layer.8.attention.self.value             | Linear            | 590 K \n",
      "159 | base_model.encoder.layer.8.attention.self.dropout           | Dropout           | 0     \n",
      "160 | base_model.encoder.layer.8.attention.output                 | BertSelfOutput    | 592 K \n",
      "161 | base_model.encoder.layer.8.attention.output.dense           | Linear            | 590 K \n",
      "162 | base_model.encoder.layer.8.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "163 | base_model.encoder.layer.8.attention.output.dropout         | Dropout           | 0     \n",
      "164 | base_model.encoder.layer.8.intermediate                     | BertIntermediate  | 2 M   \n",
      "165 | base_model.encoder.layer.8.intermediate.dense               | Linear            | 2 M   \n",
      "166 | base_model.encoder.layer.8.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "167 | base_model.encoder.layer.8.output                           | BertOutput        | 2 M   \n",
      "168 | base_model.encoder.layer.8.output.dense                     | Linear            | 2 M   \n",
      "169 | base_model.encoder.layer.8.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "170 | base_model.encoder.layer.8.output.dropout                   | Dropout           | 0     \n",
      "171 | base_model.encoder.layer.9                                  | BertLayer         | 7 M   \n",
      "172 | base_model.encoder.layer.9.attention                        | BertAttention     | 2 M   \n",
      "173 | base_model.encoder.layer.9.attention.self                   | BertSelfAttention | 1 M   \n",
      "174 | base_model.encoder.layer.9.attention.self.query             | Linear            | 590 K \n",
      "175 | base_model.encoder.layer.9.attention.self.key               | Linear            | 590 K \n",
      "176 | base_model.encoder.layer.9.attention.self.value             | Linear            | 590 K \n",
      "177 | base_model.encoder.layer.9.attention.self.dropout           | Dropout           | 0     \n",
      "178 | base_model.encoder.layer.9.attention.output                 | BertSelfOutput    | 592 K \n",
      "179 | base_model.encoder.layer.9.attention.output.dense           | Linear            | 590 K \n",
      "180 | base_model.encoder.layer.9.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "181 | base_model.encoder.layer.9.attention.output.dropout         | Dropout           | 0     \n",
      "182 | base_model.encoder.layer.9.intermediate                     | BertIntermediate  | 2 M   \n",
      "183 | base_model.encoder.layer.9.intermediate.dense               | Linear            | 2 M   \n",
      "184 | base_model.encoder.layer.9.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "185 | base_model.encoder.layer.9.output                           | BertOutput        | 2 M   \n",
      "186 | base_model.encoder.layer.9.output.dense                     | Linear            | 2 M   \n",
      "187 | base_model.encoder.layer.9.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "188 | base_model.encoder.layer.9.output.dropout                   | Dropout           | 0     \n",
      "189 | base_model.pooler                                           | BertPooler        | 590 K \n",
      "190 | base_model.pooler.dense                                     | Linear            | 590 K \n",
      "191 | base_model.pooler.activation                                | Tanh              | 0     \n",
      "192 | iterative_transformer                                       | ModuleList        | 0     \n",
      "193 | dropout_fun                                                 | Dropout           | 0     \n",
      "194 | embedding                                                   | Embedding         | 76 K  \n",
      "195 | merge_layer                                                 | Linear            | 230 K \n",
      "196 | ilabelling_layer                                            | Linear            | 1 K   \n",
      "197 | loss_fun                                                    | CrossEntropyLoss  | 0     \n",
      "INFO:lightning:\n",
      "    | Name                                                        | Type              | Params\n",
      "----------------------------------------------------------------------------------------------\n",
      "0   | base_model                                                  | BertModel         | 94 M  \n",
      "1   | base_model.embeddings                                       | BertEmbeddings    | 22 M  \n",
      "2   | base_model.embeddings.word_embeddings                       | Embedding         | 22 M  \n",
      "3   | base_model.embeddings.position_embeddings                   | Embedding         | 393 K \n",
      "4   | base_model.embeddings.token_type_embeddings                 | Embedding         | 1 K   \n",
      "5   | base_model.embeddings.LayerNorm                             | LayerNorm         | 1 K   \n",
      "6   | base_model.embeddings.dropout                               | Dropout           | 0     \n",
      "7   | base_model.encoder                                          | BertEncoder       | 70 M  \n",
      "8   | base_model.encoder.layer                                    | ModuleList        | 70 M  \n",
      "9   | base_model.encoder.layer.0                                  | BertLayer         | 7 M   \n",
      "10  | base_model.encoder.layer.0.attention                        | BertAttention     | 2 M   \n",
      "11  | base_model.encoder.layer.0.attention.self                   | BertSelfAttention | 1 M   \n",
      "12  | base_model.encoder.layer.0.attention.self.query             | Linear            | 590 K \n",
      "13  | base_model.encoder.layer.0.attention.self.key               | Linear            | 590 K \n",
      "14  | base_model.encoder.layer.0.attention.self.value             | Linear            | 590 K \n",
      "15  | base_model.encoder.layer.0.attention.self.dropout           | Dropout           | 0     \n",
      "16  | base_model.encoder.layer.0.attention.output                 | BertSelfOutput    | 592 K \n",
      "17  | base_model.encoder.layer.0.attention.output.dense           | Linear            | 590 K \n",
      "18  | base_model.encoder.layer.0.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "19  | base_model.encoder.layer.0.attention.output.dropout         | Dropout           | 0     \n",
      "20  | base_model.encoder.layer.0.intermediate                     | BertIntermediate  | 2 M   \n",
      "21  | base_model.encoder.layer.0.intermediate.dense               | Linear            | 2 M   \n",
      "22  | base_model.encoder.layer.0.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "23  | base_model.encoder.layer.0.output                           | BertOutput        | 2 M   \n",
      "24  | base_model.encoder.layer.0.output.dense                     | Linear            | 2 M   \n",
      "25  | base_model.encoder.layer.0.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "26  | base_model.encoder.layer.0.output.dropout                   | Dropout           | 0     \n",
      "27  | base_model.encoder.layer.1                                  | BertLayer         | 7 M   \n",
      "28  | base_model.encoder.layer.1.attention                        | BertAttention     | 2 M   \n",
      "29  | base_model.encoder.layer.1.attention.self                   | BertSelfAttention | 1 M   \n",
      "30  | base_model.encoder.layer.1.attention.self.query             | Linear            | 590 K \n",
      "31  | base_model.encoder.layer.1.attention.self.key               | Linear            | 590 K \n",
      "32  | base_model.encoder.layer.1.attention.self.value             | Linear            | 590 K \n",
      "33  | base_model.encoder.layer.1.attention.self.dropout           | Dropout           | 0     \n",
      "34  | base_model.encoder.layer.1.attention.output                 | BertSelfOutput    | 592 K \n",
      "35  | base_model.encoder.layer.1.attention.output.dense           | Linear            | 590 K \n",
      "36  | base_model.encoder.layer.1.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "37  | base_model.encoder.layer.1.attention.output.dropout         | Dropout           | 0     \n",
      "38  | base_model.encoder.layer.1.intermediate                     | BertIntermediate  | 2 M   \n",
      "39  | base_model.encoder.layer.1.intermediate.dense               | Linear            | 2 M   \n",
      "40  | base_model.encoder.layer.1.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "41  | base_model.encoder.layer.1.output                           | BertOutput        | 2 M   \n",
      "42  | base_model.encoder.layer.1.output.dense                     | Linear            | 2 M   \n",
      "43  | base_model.encoder.layer.1.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "44  | base_model.encoder.layer.1.output.dropout                   | Dropout           | 0     \n",
      "45  | base_model.encoder.layer.2                                  | BertLayer         | 7 M   \n",
      "46  | base_model.encoder.layer.2.attention                        | BertAttention     | 2 M   \n",
      "47  | base_model.encoder.layer.2.attention.self                   | BertSelfAttention | 1 M   \n",
      "48  | base_model.encoder.layer.2.attention.self.query             | Linear            | 590 K \n",
      "49  | base_model.encoder.layer.2.attention.self.key               | Linear            | 590 K \n",
      "50  | base_model.encoder.layer.2.attention.self.value             | Linear            | 590 K \n",
      "51  | base_model.encoder.layer.2.attention.self.dropout           | Dropout           | 0     \n",
      "52  | base_model.encoder.layer.2.attention.output                 | BertSelfOutput    | 592 K \n",
      "53  | base_model.encoder.layer.2.attention.output.dense           | Linear            | 590 K \n",
      "54  | base_model.encoder.layer.2.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "55  | base_model.encoder.layer.2.attention.output.dropout         | Dropout           | 0     \n",
      "56  | base_model.encoder.layer.2.intermediate                     | BertIntermediate  | 2 M   \n",
      "57  | base_model.encoder.layer.2.intermediate.dense               | Linear            | 2 M   \n",
      "58  | base_model.encoder.layer.2.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "59  | base_model.encoder.layer.2.output                           | BertOutput        | 2 M   \n",
      "60  | base_model.encoder.layer.2.output.dense                     | Linear            | 2 M   \n",
      "61  | base_model.encoder.layer.2.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "62  | base_model.encoder.layer.2.output.dropout                   | Dropout           | 0     \n",
      "63  | base_model.encoder.layer.3                                  | BertLayer         | 7 M   \n",
      "64  | base_model.encoder.layer.3.attention                        | BertAttention     | 2 M   \n",
      "65  | base_model.encoder.layer.3.attention.self                   | BertSelfAttention | 1 M   \n",
      "66  | base_model.encoder.layer.3.attention.self.query             | Linear            | 590 K \n",
      "67  | base_model.encoder.layer.3.attention.self.key               | Linear            | 590 K \n",
      "68  | base_model.encoder.layer.3.attention.self.value             | Linear            | 590 K \n",
      "69  | base_model.encoder.layer.3.attention.self.dropout           | Dropout           | 0     \n",
      "70  | base_model.encoder.layer.3.attention.output                 | BertSelfOutput    | 592 K \n",
      "71  | base_model.encoder.layer.3.attention.output.dense           | Linear            | 590 K \n",
      "72  | base_model.encoder.layer.3.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "73  | base_model.encoder.layer.3.attention.output.dropout         | Dropout           | 0     \n",
      "74  | base_model.encoder.layer.3.intermediate                     | BertIntermediate  | 2 M   \n",
      "75  | base_model.encoder.layer.3.intermediate.dense               | Linear            | 2 M   \n",
      "76  | base_model.encoder.layer.3.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "77  | base_model.encoder.layer.3.output                           | BertOutput        | 2 M   \n",
      "78  | base_model.encoder.layer.3.output.dense                     | Linear            | 2 M   \n",
      "79  | base_model.encoder.layer.3.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "80  | base_model.encoder.layer.3.output.dropout                   | Dropout           | 0     \n",
      "81  | base_model.encoder.layer.4                                  | BertLayer         | 7 M   \n",
      "82  | base_model.encoder.layer.4.attention                        | BertAttention     | 2 M   \n",
      "83  | base_model.encoder.layer.4.attention.self                   | BertSelfAttention | 1 M   \n",
      "84  | base_model.encoder.layer.4.attention.self.query             | Linear            | 590 K \n",
      "85  | base_model.encoder.layer.4.attention.self.key               | Linear            | 590 K \n",
      "86  | base_model.encoder.layer.4.attention.self.value             | Linear            | 590 K \n",
      "87  | base_model.encoder.layer.4.attention.self.dropout           | Dropout           | 0     \n",
      "88  | base_model.encoder.layer.4.attention.output                 | BertSelfOutput    | 592 K \n",
      "89  | base_model.encoder.layer.4.attention.output.dense           | Linear            | 590 K \n",
      "90  | base_model.encoder.layer.4.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "91  | base_model.encoder.layer.4.attention.output.dropout         | Dropout           | 0     \n",
      "92  | base_model.encoder.layer.4.intermediate                     | BertIntermediate  | 2 M   \n",
      "93  | base_model.encoder.layer.4.intermediate.dense               | Linear            | 2 M   \n",
      "94  | base_model.encoder.layer.4.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "95  | base_model.encoder.layer.4.output                           | BertOutput        | 2 M   \n",
      "96  | base_model.encoder.layer.4.output.dense                     | Linear            | 2 M   \n",
      "97  | base_model.encoder.layer.4.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "98  | base_model.encoder.layer.4.output.dropout                   | Dropout           | 0     \n",
      "99  | base_model.encoder.layer.5                                  | BertLayer         | 7 M   \n",
      "100 | base_model.encoder.layer.5.attention                        | BertAttention     | 2 M   \n",
      "101 | base_model.encoder.layer.5.attention.self                   | BertSelfAttention | 1 M   \n",
      "102 | base_model.encoder.layer.5.attention.self.query             | Linear            | 590 K \n",
      "103 | base_model.encoder.layer.5.attention.self.key               | Linear            | 590 K \n",
      "104 | base_model.encoder.layer.5.attention.self.value             | Linear            | 590 K \n",
      "105 | base_model.encoder.layer.5.attention.self.dropout           | Dropout           | 0     \n",
      "106 | base_model.encoder.layer.5.attention.output                 | BertSelfOutput    | 592 K \n",
      "107 | base_model.encoder.layer.5.attention.output.dense           | Linear            | 590 K \n",
      "108 | base_model.encoder.layer.5.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "109 | base_model.encoder.layer.5.attention.output.dropout         | Dropout           | 0     \n",
      "110 | base_model.encoder.layer.5.intermediate                     | BertIntermediate  | 2 M   \n",
      "111 | base_model.encoder.layer.5.intermediate.dense               | Linear            | 2 M   \n",
      "112 | base_model.encoder.layer.5.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "113 | base_model.encoder.layer.5.output                           | BertOutput        | 2 M   \n",
      "114 | base_model.encoder.layer.5.output.dense                     | Linear            | 2 M   \n",
      "115 | base_model.encoder.layer.5.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "116 | base_model.encoder.layer.5.output.dropout                   | Dropout           | 0     \n",
      "117 | base_model.encoder.layer.6                                  | BertLayer         | 7 M   \n",
      "118 | base_model.encoder.layer.6.attention                        | BertAttention     | 2 M   \n",
      "119 | base_model.encoder.layer.6.attention.self                   | BertSelfAttention | 1 M   \n",
      "120 | base_model.encoder.layer.6.attention.self.query             | Linear            | 590 K \n",
      "121 | base_model.encoder.layer.6.attention.self.key               | Linear            | 590 K \n",
      "122 | base_model.encoder.layer.6.attention.self.value             | Linear            | 590 K \n",
      "123 | base_model.encoder.layer.6.attention.self.dropout           | Dropout           | 0     \n",
      "124 | base_model.encoder.layer.6.attention.output                 | BertSelfOutput    | 592 K \n",
      "125 | base_model.encoder.layer.6.attention.output.dense           | Linear            | 590 K \n",
      "126 | base_model.encoder.layer.6.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "127 | base_model.encoder.layer.6.attention.output.dropout         | Dropout           | 0     \n",
      "128 | base_model.encoder.layer.6.intermediate                     | BertIntermediate  | 2 M   \n",
      "129 | base_model.encoder.layer.6.intermediate.dense               | Linear            | 2 M   \n",
      "130 | base_model.encoder.layer.6.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "131 | base_model.encoder.layer.6.output                           | BertOutput        | 2 M   \n",
      "132 | base_model.encoder.layer.6.output.dense                     | Linear            | 2 M   \n",
      "133 | base_model.encoder.layer.6.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "134 | base_model.encoder.layer.6.output.dropout                   | Dropout           | 0     \n",
      "135 | base_model.encoder.layer.7                                  | BertLayer         | 7 M   \n",
      "136 | base_model.encoder.layer.7.attention                        | BertAttention     | 2 M   \n",
      "137 | base_model.encoder.layer.7.attention.self                   | BertSelfAttention | 1 M   \n",
      "138 | base_model.encoder.layer.7.attention.self.query             | Linear            | 590 K \n",
      "139 | base_model.encoder.layer.7.attention.self.key               | Linear            | 590 K \n",
      "140 | base_model.encoder.layer.7.attention.self.value             | Linear            | 590 K \n",
      "141 | base_model.encoder.layer.7.attention.self.dropout           | Dropout           | 0     \n",
      "142 | base_model.encoder.layer.7.attention.output                 | BertSelfOutput    | 592 K \n",
      "143 | base_model.encoder.layer.7.attention.output.dense           | Linear            | 590 K \n",
      "144 | base_model.encoder.layer.7.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "145 | base_model.encoder.layer.7.attention.output.dropout         | Dropout           | 0     \n",
      "146 | base_model.encoder.layer.7.intermediate                     | BertIntermediate  | 2 M   \n",
      "147 | base_model.encoder.layer.7.intermediate.dense               | Linear            | 2 M   \n",
      "148 | base_model.encoder.layer.7.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "149 | base_model.encoder.layer.7.output                           | BertOutput        | 2 M   \n",
      "150 | base_model.encoder.layer.7.output.dense                     | Linear            | 2 M   \n",
      "151 | base_model.encoder.layer.7.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "152 | base_model.encoder.layer.7.output.dropout                   | Dropout           | 0     \n",
      "153 | base_model.encoder.layer.8                                  | BertLayer         | 7 M   \n",
      "154 | base_model.encoder.layer.8.attention                        | BertAttention     | 2 M   \n",
      "155 | base_model.encoder.layer.8.attention.self                   | BertSelfAttention | 1 M   \n",
      "156 | base_model.encoder.layer.8.attention.self.query             | Linear            | 590 K \n",
      "157 | base_model.encoder.layer.8.attention.self.key               | Linear            | 590 K \n",
      "158 | base_model.encoder.layer.8.attention.self.value             | Linear            | 590 K \n",
      "159 | base_model.encoder.layer.8.attention.self.dropout           | Dropout           | 0     \n",
      "160 | base_model.encoder.layer.8.attention.output                 | BertSelfOutput    | 592 K \n",
      "161 | base_model.encoder.layer.8.attention.output.dense           | Linear            | 590 K \n",
      "162 | base_model.encoder.layer.8.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "163 | base_model.encoder.layer.8.attention.output.dropout         | Dropout           | 0     \n",
      "164 | base_model.encoder.layer.8.intermediate                     | BertIntermediate  | 2 M   \n",
      "165 | base_model.encoder.layer.8.intermediate.dense               | Linear            | 2 M   \n",
      "166 | base_model.encoder.layer.8.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "167 | base_model.encoder.layer.8.output                           | BertOutput        | 2 M   \n",
      "168 | base_model.encoder.layer.8.output.dense                     | Linear            | 2 M   \n",
      "169 | base_model.encoder.layer.8.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "170 | base_model.encoder.layer.8.output.dropout                   | Dropout           | 0     \n",
      "171 | base_model.encoder.layer.9                                  | BertLayer         | 7 M   \n",
      "172 | base_model.encoder.layer.9.attention                        | BertAttention     | 2 M   \n",
      "173 | base_model.encoder.layer.9.attention.self                   | BertSelfAttention | 1 M   \n",
      "174 | base_model.encoder.layer.9.attention.self.query             | Linear            | 590 K \n",
      "175 | base_model.encoder.layer.9.attention.self.key               | Linear            | 590 K \n",
      "176 | base_model.encoder.layer.9.attention.self.value             | Linear            | 590 K \n",
      "177 | base_model.encoder.layer.9.attention.self.dropout           | Dropout           | 0     \n",
      "178 | base_model.encoder.layer.9.attention.output                 | BertSelfOutput    | 592 K \n",
      "179 | base_model.encoder.layer.9.attention.output.dense           | Linear            | 590 K \n",
      "180 | base_model.encoder.layer.9.attention.output.LayerNorm       | LayerNorm         | 1 K   \n",
      "181 | base_model.encoder.layer.9.attention.output.dropout         | Dropout           | 0     \n",
      "182 | base_model.encoder.layer.9.intermediate                     | BertIntermediate  | 2 M   \n",
      "183 | base_model.encoder.layer.9.intermediate.dense               | Linear            | 2 M   \n",
      "184 | base_model.encoder.layer.9.intermediate.intermediate_act_fn | GELUActivation    | 0     \n",
      "185 | base_model.encoder.layer.9.output                           | BertOutput        | 2 M   \n",
      "186 | base_model.encoder.layer.9.output.dense                     | Linear            | 2 M   \n",
      "187 | base_model.encoder.layer.9.output.LayerNorm                 | LayerNorm         | 1 K   \n",
      "188 | base_model.encoder.layer.9.output.dropout                   | Dropout           | 0     \n",
      "189 | base_model.pooler                                           | BertPooler        | 590 K \n",
      "190 | base_model.pooler.dense                                     | Linear            | 590 K \n",
      "191 | base_model.pooler.activation                                | Tanh              | 0     \n",
      "192 | iterative_transformer                                       | ModuleList        | 0     \n",
      "193 | dropout_fun                                                 | Dropout           | 0     \n",
      "194 | embedding                                                   | Embedding         | 76 K  \n",
      "195 | merge_layer                                                 | Linear            | 230 K \n",
      "196 | ilabelling_layer                                            | Linear            | 1 K   \n",
      "197 | loss_fun                                                    | CrossEntropyLoss  | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce0a9fda44843a093b4d94fb500bb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering 'training_step' method, batch_idx=0\n",
      "Entering 'training_step' method, batch_idx=1\n",
      "Entering 'training_step' method, batch_idx=2\n",
      "Entering 'training_step' method, batch_idx=3\n",
      "Entering 'training_step' method, batch_idx=4\n",
      "Entering 'training_step' method, batch_idx=5\n",
      "Entering 'training_step' method, batch_idx=6\n",
      "Entering 'training_step' method, batch_idx=7\n",
      "Entering 'training_step' method, batch_idx=8\n",
      "Entering 'training_step' method, batch_idx=9\n",
      "Entering 'training_step' method, batch_idx=10\n",
      "Entering 'training_step' method, batch_idx=11\n",
      "Entering 'training_step' method, batch_idx=12\n",
      "Entering 'training_step' method, batch_idx=13\n",
      "Entering 'training_step' method, batch_idx=14\n",
      "Entering 'training_step' method, batch_idx=15\n",
      "Entering 'training_step' method, batch_idx=16\n",
      "Entering 'training_step' method, batch_idx=17\n",
      "Entering 'training_step' method, batch_idx=18\n",
      "Entering 'training_step' method, batch_idx=19\n",
      "Entering 'training_step' method, batch_idx=20\n",
      "Entering 'training_step' method, batch_idx=21\n",
      "Entering 'training_step' method, batch_idx=22\n",
      "Entering 'training_step' method, batch_idx=23\n",
      "Entering 'training_step' method, batch_idx=24\n",
      "Entering 'training_step' method, batch_idx=25\n",
      "Entering 'training_step' method, batch_idx=26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c3539032a64e45a6a4bcc865a1687b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering 'validation_step' method, batch_idx=0\n",
      "ll_max_log_prob torch.Size([24, 66])\n",
      "tensor([[-0.0009, -0.0009, -0.0009,  ..., -0.0009, -0.0009, -0.0009],\n",
      "        [-0.0010, -0.0010, -0.0010,  ..., -0.0010, -0.0011, -0.0014],\n",
      "        [-0.0009, -0.0009, -0.0009,  ..., -0.0009, -0.0009, -0.0009],\n",
      "        ...,\n",
      "        [-0.0010, -0.0010, -0.0010,  ..., -0.0010, -0.0010, -0.0010],\n",
      "        [-0.0009, -0.0009, -0.0009,  ..., -0.0009, -0.0009, -0.0009],\n",
      "        [-0.0009, -0.0009, -0.0009,  ..., -0.0009, -0.0009, -0.0009]])\n",
      "ll_pred_ilabel torch.Size([24, 66])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (66) must match the size of tensor b (84) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_168/3430799317.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_168/2914028585.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(pid)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gpus\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mconductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMConductor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mconductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/sagemaker-studiolab-notebooks/SentenceAx/MConductor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, pred_in_fp)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_in_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sagemaker-studiolab-notebooks/SentenceAx/MConductor.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m                                    \u001b[0mcheckpoint_fp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                                    use_minimal=False)\n\u001b[0;32m--> 305\u001b[0;31m         trainer.fit(\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdloader_tool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_frequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;31m# return 1 when finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m     def test(\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# fast_dev_run always forces val checking after train batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_dev_run\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mshould_check_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_checkpoint_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_early_stop_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, test_mode)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_bar_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, model, dataloaders, max_batches, test_mode)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;31m# on dp / ddp2 might still want to do something with the batch parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_forward\u001b[0;34m(self, model, batch, batch_idx, dataloader_idx, test_mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sagemaker-studiolab-notebooks/SentenceAx/Model.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \"\"\"\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msax_ttt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tune\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sagemaker-studiolab-notebooks/SentenceAx/Model.py\u001b[0m in \u001b[0;36msax_ttt_step\u001b[0;34m(self, batch, batch_idx, ttt)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 batch_idx))\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0mbatch_m_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mttt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tune\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sagemaker-studiolab-notebooks/SentenceAx/Model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, batch_idx, ttt)\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;31m# outsource everything after do loop to a new function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         return self.sax_calc_forward_output(\n\u001b[0m\u001b[1;32m    425\u001b[0m             \u001b[0mx_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mttt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sagemaker-studiolab-notebooks/SentenceAx/Model.py\u001b[0m in \u001b[0;36msax_calc_forward_output\u001b[0;34m(self, x_d, y_d, meta_d, ttt, lll_word_score, llll_word_score)\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0;31m# * is element-wise multiplication of tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mll_nonpad_bool\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0mll_pred_ilabel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mll_nonpad_bool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m                 \u001b[0mll_norm_log_prob\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mll_max_log_prob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mll_nonpad_bool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (66) must match the size of tensor b (84) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "main(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
