{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21bfe53-38da-4383-8f22-72a6612a24f0",
   "metadata": {},
   "source": [
    "# warmup cc-train_test(pid=5)\n",
    "\n",
    "SentenceAx uses 2 NNs, one for task=\"ex\" and another for task=\"cc\". This warmup notebook trains the NN for the task=\"cc\". \n",
    "\n",
    "The warmup NN has small sizes for everything so that it can be trained quickly but not accurately without GPU.\n",
    "\n",
    "<font color='red'>**After running this notebook, append the suffix \".best\" to the checkpoint file (in the `weights_warmup/cc_model` directory) that this notebook outputs. Otherwise, the checkpoint generated by this run of this notebook will be erased in the next run. Furthermore, notebooks for predicting that need a weights file, won't find one, as they are designed to look for a weights file whose name ends in \"best\".**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e3a604-724d-4fa1-bdc1-486cc8322d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rrtuc\\Desktop\\backed-up\\python-projects\\SentenceAx\n"
     ]
    }
   ],
   "source": [
    "# this makes sure it starts looking for things from the SentenceAx folder down.\n",
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.insert(0,os.getcwd())\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752f3d00-67db-4e93-bf08-7169fe092e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(os.environ[\"TOKENIZERS_PARALLELISM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91bf119f-ae39-4070-b9b3-c5cfe13b0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Params import *\n",
    "from ActionConductor import *\n",
    "\n",
    "\n",
    "def main(pid, pred_in_fp=None, split_only=False):\n",
    "    params = Params(pid)\n",
    "    params.d[\"refresh_cache\"] = True\n",
    "    params.d[\"gpus\"] = 0\n",
    "    params.d[\"batch_size\"] = 4\n",
    "    params.d[\"cache_dir\"] = \"cache_warmup\"\n",
    "    params.d[\"logs_dir\"] = \"logs_warmup\"\n",
    "    params.d[\"num_epochs\"] = 3\n",
    "    params.d[\"num_steps_per_epoch\"] = 3\n",
    "    params.d[\"model_str\"] = \"bert-base-cased\"\n",
    "    params.d[\"small_train\"] = True\n",
    "    params.d[\"weights_dir\"] = \"weights_warmup\"\n",
    "    params.describe_self()\n",
    "    \n",
    "    conductor = ActionConductor(params, verbose=True)\n",
    "    conductor.delete_all_checkpoints()\n",
    "    print(\"checkpoints:\", conductor.get_all_checkpoint_fp())\n",
    "    conductor.run(pred_in_fp, split_only)\n",
    "    print(\"checkpoints:\", conductor.get_all_checkpoint_fp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfed0d9d-76ea-4231-9fab-1e8d0e9b5a67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** new params\n",
      "new params: pid=5, task='cc', action='train_test'\n",
      "params=\n",
      "{'accumulate_grad_batches': 1,\n",
      " 'action': 'train_test',\n",
      " 'batch_size': 4,\n",
      " 'cache_dir': 'cache_warmup',\n",
      " 'con_weight_str': '1',\n",
      " 'dropout_fun': 0.0,\n",
      " 'gpus': 0,\n",
      " 'gradient_clip_val': 5,\n",
      " 'logs_dir': 'logs_warmup',\n",
      " 'lr': 2e-05,\n",
      " 'model_str': 'bert-base-cased',\n",
      " 'num_epochs': 3,\n",
      " 'num_iterative_layers': 2,\n",
      " 'num_steps_per_epoch': 3,\n",
      " 'optimizer': 'adamW',\n",
      " 'refresh_cache': True,\n",
      " 'save_k': 1,\n",
      " 'small_train': True,\n",
      " 'task': 'cc',\n",
      " 'val_check_interval': 1.0,\n",
      " 'verbose': False,\n",
      " 'weights_dir': 'weights_warmup',\n",
      " 'wreg': 0}\n",
      "lightning version is 2.1.0 so it is >= 2.0.1 as required.\n",
      "SEED= 777\n",
      "\n",
      "MInput started reading 'tests/small_cctags.txt'\n",
      "...\n",
      "1. Line 16 has no valid extractions.\n",
      "2. Line 111 has no valid extractions.\n",
      "3. Line 116 has no valid extractions.\n",
      "4. Line 246 has no valid extractions.\n",
      "5. Line 251 has no valid extractions.\n",
      "6. Line 311 has no valid extractions.\n",
      "7. Line 316 has no valid extractions.\n",
      "8. Line 321 has no valid extractions.\n",
      "9. Line 461 has no valid extractions.\n",
      "10. Line 516 has no valid extractions.\n",
      "11. Line 591 has no valid extractions.\n",
      "12. Line 911 has no valid extractions.\n",
      "13. Line 946 has no valid extractions.\n",
      "MInput finished reading 'tests/small_cctags.txt'\n",
      "number of lines= 995\n",
      "number of used samples=  186\n",
      "number of omitted samples=  13\n",
      "\n",
      "\n",
      "MInput started reading 'input_data/openie-data/ptb-dev.labels'\n",
      "...\n",
      "1. Line 16 has no valid extractions.\n",
      "2. Line 111 has no valid extractions.\n",
      "3. Line 116 has no valid extractions.\n",
      "4. Line 246 has no valid extractions.\n",
      "5. Line 251 has no valid extractions.\n",
      "6. Line 311 has no valid extractions.\n",
      "7. Line 316 has no valid extractions.\n",
      "8. Line 321 has no valid extractions.\n",
      "9. Line 461 has no valid extractions.\n",
      "10. Line 516 has no valid extractions.\n",
      "11. Line 591 has no valid extractions.\n",
      "12. Line 911 has no valid extractions.\n",
      "13. Line 946 has no valid extractions.\n",
      "14. Line 1021 has no valid extractions.\n",
      "15. Line 1061 has no valid extractions.\n",
      "16. Line 1106 has no valid extractions.\n",
      "17. Line 1161 has no valid extractions.\n",
      "18. Line 1181 has no valid extractions.\n",
      "19. Line 1206 has no valid extractions.\n",
      "20. Line 1226 has no valid extractions.\n",
      "21. Line 1236 has no valid extractions.\n",
      "22. Line 1241 has no valid extractions.\n",
      "23. Line 1286 has no valid extractions.\n",
      "24. Line 1291 has no valid extractions.\n",
      "25. Line 1306 has > 100 words. length=117. first 60 words:\n",
      "It would like to peg the ceiling on Federal Housing Administ\n",
      "26. Line 1356 has no valid extractions.\n",
      "27. Line 1366 has no valid extractions.\n",
      "28. Line 1381 has no valid extractions.\n",
      "29. Line 1501 has no valid extractions.\n",
      "30. Line 1526 has no valid extractions.\n",
      "31. Line 1541 has no valid extractions.\n",
      "32. Line 1561 has no valid extractions.\n",
      "33. Line 1571 has no valid extractions.\n",
      "34. Line 1621 has no valid extractions.\n",
      "35. Line 1631 has no valid extractions.\n",
      "36. Line 1696 has no valid extractions.\n",
      "37. Line 1766 has no valid extractions.\n",
      "38. Line 1796 has no valid extractions.\n",
      "39. Line 1806 has no valid extractions.\n",
      "40. Line 1841 has no valid extractions.\n",
      "41. Line 1881 has no valid extractions.\n",
      "42. Line 1961 has no valid extractions.\n",
      "43. Line 1976 has no valid extractions.\n",
      "44. Line 1996 has no valid extractions.\n",
      "45. Line 2011 has no valid extractions.\n",
      "46. Line 2026 has no valid extractions.\n",
      "47. Line 2121 has no valid extractions.\n",
      "48. Line 2151 has no valid extractions.\n",
      "49. Line 2191 has no valid extractions.\n",
      "50. Line 2246 has no valid extractions.\n",
      "51. Line 2431 has no valid extractions.\n",
      "52. Line 2446 has no valid extractions.\n",
      "53. Line 2561 has no valid extractions.\n",
      "54. Line 2571 has no valid extractions.\n",
      "55. Line 2661 has no valid extractions.\n",
      "56. Line 2791 has no valid extractions.\n",
      "57. Line 2796 has no valid extractions.\n",
      "58. Line 2801 has no valid extractions.\n",
      "59. Line 3011 has no valid extractions.\n",
      "60. Line 3026 has no valid extractions.\n",
      "61. Line 3211 has no valid extractions.\n",
      "62. Line 3246 has no valid extractions.\n",
      "63. Line 3301 has no valid extractions.\n",
      "64. Line 3306 has no valid extractions.\n",
      "65. Line 3311 has no valid extractions.\n",
      "66. Line 3341 has no valid extractions.\n",
      "67. Line 3376 has no valid extractions.\n",
      "68. Line 3616 has no valid extractions.\n",
      "69. Line 3641 has no valid extractions.\n",
      "70. Line 3671 has no valid extractions.\n",
      "MInput finished reading 'input_data/openie-data/ptb-dev.labels'\n",
      "number of lines= 3710\n",
      "number of used samples=  672\n",
      "number of omitted samples=  70\n",
      "\n",
      "\n",
      "MInput started reading 'input_data/openie-data/ptb-test.labels'\n",
      "...\n",
      "1. Line 1 has no valid extractions.\n",
      "2. Line 46 has no valid extractions.\n",
      "3. Line 91 has no valid extractions.\n",
      "4. Line 131 has no valid extractions.\n",
      "5. Line 136 has no valid extractions.\n",
      "6. Line 156 has no valid extractions.\n",
      "7. Line 161 has no valid extractions.\n",
      "8. Line 296 has no valid extractions.\n",
      "9. Line 321 has no valid extractions.\n",
      "10. Line 356 has no valid extractions.\n",
      "11. Line 361 has no valid extractions.\n",
      "12. Line 496 has no valid extractions.\n",
      "13. Line 611 has no valid extractions.\n",
      "14. Line 656 has no valid extractions.\n",
      "15. Line 686 has no valid extractions.\n",
      "16. Line 751 has no valid extractions.\n",
      "17. Line 846 has no valid extractions.\n",
      "18. Line 851 has no valid extractions.\n",
      "19. Line 856 has no valid extractions.\n",
      "20. Line 901 has no valid extractions.\n",
      "21. Line 906 has no valid extractions.\n",
      "22. Line 951 has no valid extractions.\n",
      "23. Line 1081 has no valid extractions.\n",
      "24. Line 1151 has no valid extractions.\n",
      "25. Line 1166 has no valid extractions.\n",
      "26. Line 1171 has no valid extractions.\n",
      "27. Line 1186 has no valid extractions.\n",
      "28. Line 1201 has no valid extractions.\n",
      "29. Line 1241 has no valid extractions.\n",
      "30. Line 1276 has no valid extractions.\n",
      "31. Line 1286 has no valid extractions.\n",
      "32. Line 1291 has no valid extractions.\n",
      "33. Line 1796 has no valid extractions.\n",
      "34. Line 1891 has no valid extractions.\n",
      "35. Line 1981 has no valid extractions.\n",
      "36. Line 1986 has no valid extractions.\n",
      "37. Line 2016 has no valid extractions.\n",
      "38. Line 2031 has no valid extractions.\n",
      "39. Line 2046 has no valid extractions.\n",
      "40. Line 2066 has no valid extractions.\n",
      "41. Line 2096 has no valid extractions.\n",
      "42. Line 2101 has no valid extractions.\n",
      "43. Line 2141 has no valid extractions.\n",
      "44. Line 2216 has no valid extractions.\n",
      "45. Line 2236 has no valid extractions.\n",
      "46. Line 2261 has no valid extractions.\n",
      "47. Line 2271 has no valid extractions.\n",
      "48. Line 2296 has no valid extractions.\n",
      "49. Line 2301 has no valid extractions.\n",
      "50. Line 2376 has no valid extractions.\n",
      "51. Line 2386 has no valid extractions.\n",
      "52. Line 2391 has no valid extractions.\n",
      "53. Line 2446 has no valid extractions.\n",
      "54. Line 2466 has no valid extractions.\n",
      "55. Line 2551 has no valid extractions.\n",
      "56. Line 2571 has no valid extractions.\n",
      "57. Line 2586 has no valid extractions.\n",
      "58. Line 2711 has no valid extractions.\n",
      "59. Line 2721 has no valid extractions.\n",
      "60. Line 2761 has no valid extractions.\n",
      "61. Line 2791 has no valid extractions.\n",
      "62. Line 2966 has no valid extractions.\n",
      "63. Line 3011 has no valid extractions.\n",
      "64. Line 3086 has no valid extractions.\n",
      "65. Line 3136 has no valid extractions.\n",
      "66. Line 3186 has no valid extractions.\n",
      "67. Line 3226 has no valid extractions.\n",
      "68. Line 3251 has no valid extractions.\n",
      "69. Line 3271 has no valid extractions.\n",
      "70. Line 3276 has no valid extractions.\n",
      "71. Line 3286 has no valid extractions.\n",
      "72. Line 3331 has no valid extractions.\n",
      "73. Line 3356 has no valid extractions.\n",
      "74. Line 3411 has no valid extractions.\n",
      "75. Line 3416 has no valid extractions.\n",
      "76. Line 3451 has no valid extractions.\n",
      "77. Line 3581 has no valid extractions.\n",
      "78. Line 3626 has no valid extractions.\n",
      "79. Line 3656 has no valid extractions.\n",
      "80. Line 3661 has no valid extractions.\n",
      "81. Line 3671 has no valid extractions.\n",
      "82. Line 3766 has no valid extractions.\n",
      "83. Line 3776 has no valid extractions.\n",
      "84. Line 3841 has no valid extractions.\n",
      "85. Line 3856 has no valid extractions.\n",
      "86. Line 3861 has no valid extractions.\n",
      "87. Line 3911 has no valid extractions.\n",
      "88. Line 3921 has no valid extractions.\n",
      "89. Line 3936 has no valid extractions.\n",
      "90. Line 3946 has no valid extractions.\n",
      "91. Line 3976 has no valid extractions.\n",
      "92. Line 4006 has no valid extractions.\n",
      "93. Line 4011 has no valid extractions.\n",
      "94. Line 4021 has no valid extractions.\n",
      "95. Line 4036 has no valid extractions.\n",
      "96. Line 4106 has no valid extractions.\n",
      "97. Line 4146 has no valid extractions.\n",
      "98. Line 4176 has no valid extractions.\n",
      "99. Line 4196 has no valid extractions.\n",
      "100. Line 4266 has no valid extractions.\n",
      "101. Line 4311 has no valid extractions.\n",
      "102. Line 4366 has no valid extractions.\n",
      "103. Line 4376 has no valid extractions.\n",
      "104. Line 4391 has no valid extractions.\n",
      "105. Line 4576 has no valid extractions.\n",
      "106. Line 4701 has no valid extractions.\n",
      "107. Line 4736 has no valid extractions.\n",
      "108. Line 4766 has no valid extractions.\n",
      "109. Line 4786 has no valid extractions.\n",
      "110. Line 4846 has no valid extractions.\n",
      "111. Line 4856 has no valid extractions.\n",
      "112. Line 4871 has no valid extractions.\n",
      "MInput finished reading 'input_data/openie-data/ptb-test.labels'\n",
      "number of lines= 4925\n",
      "number of used samples=  873\n",
      "number of omitted samples=  112\n",
      "\n",
      "checkpoints: []\n",
      "Saving self.hparams=  \"pi_test\": 3.14\n",
      "Model init\n",
      "\tname=train, hidden_size=768\n",
      "num_iterative_layers=  2\n",
      "num_encoder_layers=  10\n",
      "total num layers=  12\n",
      "CCMetric deleting previous pkl files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                  | Type             | Params\n",
      "-----------------------------------------------------------\n",
      "0 | base_model            | BertModel        | 94.1 M\n",
      "1 | iterative_transformer | ModuleList       | 14.2 M\n",
      "2 | dropout_fun           | Dropout          | 0     \n",
      "3 | embedding             | Embedding        | 76.8 K\n",
      "4 | merge_layer           | Linear           | 230 K \n",
      "5 | ilabelling_layer      | Linear           | 1.8 K \n",
      "6 | loss_fun              | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "434.478   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d06dbe2835843f6a656cf10034a9b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                               | 0/? [00:00<?, ?it/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 85])\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 4->5\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 5->6\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 6->7\n",
      "\n",
      "After dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 7->8\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 8->9\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 9->10\n",
      "\n",
      "After dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=3\n",
      "\tllll_word_score[0].shape=torch.Size([4, 64, 6])\n",
      "Inside Model.training_step method, batch_idx=0 {'train_loss': 5.0889, 'loss': 5.0889}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 85])\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 4->5\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 5->6\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 6->7\n",
      "\n",
      "After dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 7->8\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 8->9\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 9->10\n",
      "\n",
      "After dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=3\n",
      "\tllll_word_score[0].shape=torch.Size([4, 64, 6])\n",
      "Inside Model.training_step method, batch_idx=1 {'train_loss': 4.0236, 'loss': 4.0236}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 85])\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 4->5\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 5->6\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 6->7\n",
      "\n",
      "After dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 7->8\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 8->9\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 9->10\n",
      "\n",
      "After dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=3\n",
      "\tllll_word_score[0].shape=torch.Size([4, 64, 6])\n",
      "Inside Model.training_step method, batch_idx=2 {'train_loss': 3.3682, 'loss': 3.3682}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                             | 0/? [00:00<?, ?it/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 115])\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 4->5\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 5->6\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 6->7\n",
      "\n",
      "After dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 68])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 68])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=2\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.validation_step method, batch_idx=0 {'tune_loss': 0.0}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 115])\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 4->5\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 5->6\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 6->7\n",
      "\n",
      "After dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 68])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 68])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=2\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.validation_step method, batch_idx=1 {'tune_loss': 0.0}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 115])\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=1\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.validation_step method, batch_idx=2 {'tune_loss': 0.0}\n",
      "Entering Model.on_validation_epoch_end method\n",
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n",
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n",
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 3: 'tune_epoch_acc' reached 0.00000 (best 0.00000), saving model to 'C:\\\\Users\\\\rrtuc\\\\Desktop\\\\backed-up\\\\python-projects\\\\SentenceAx\\\\weights_warmup\\\\cc_model\\\\epoch=00_tune_epoch_acc=0.0000.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering CCMetric.get_score_d method.\n",
      "\n",
      "Scores at end of epoch 0:\n",
      "{'F1_exact': 0.0,\n",
      " 'F1_inner': 0.0,\n",
      " 'F1_outer': 0.0,\n",
      " 'F1_whole': 0.0,\n",
      " 'P_exact': 0.0,\n",
      " 'R_exact': 0.0,\n",
      " 'epoch_acc': 0.0}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 85])\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 4->5\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 5->6\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 6->7\n",
      "\n",
      "After dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 7->8\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 8->9\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 9->10\n",
      "\n",
      "After dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=3\n",
      "\tllll_word_score[0].shape=torch.Size([4, 64, 6])\n",
      "Inside Model.training_step method, batch_idx=0 {'train_loss': 2.6551, 'loss': 2.6551}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 85])\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 4->5\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 5->6\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 6->7\n",
      "\n",
      "After dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 7->8\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 8->9\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 9->10\n",
      "\n",
      "After dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=3\n",
      "\tllll_word_score[0].shape=torch.Size([4, 64, 6])\n",
      "Inside Model.training_step method, batch_idx=1 {'train_loss': 3.0654, 'loss': 3.0654}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 85])\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 4->5\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 5->6\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 6->7\n",
      "\n",
      "After dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 7->8\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 8->9\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 9->10\n",
      "\n",
      "After dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=3\n",
      "\tllll_word_score[0].shape=torch.Size([4, 64, 6])\n",
      "Inside Model.training_step method, batch_idx=2 {'train_loss': 1.6797, 'loss': 1.6797}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                             | 0/? [00:00<?, ?it/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 115])\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=1\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.validation_step method, batch_idx=0 {'tune_loss': 0.0}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 115])\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=1\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.validation_step method, batch_idx=1 {'tune_loss': 0.0}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 115])\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=1\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.validation_step method, batch_idx=2 {'tune_loss': 0.0}\n",
      "Entering Model.on_validation_epoch_end method\n",
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n",
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 6: 'tune_epoch_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n",
      "Entering CCMetric.get_score_d method.\n",
      "\n",
      "Scores at end of epoch 1:\n",
      "{'F1_exact': 0.0,\n",
      " 'F1_inner': 0.0,\n",
      " 'F1_outer': 0.0,\n",
      " 'F1_whole': 0.0,\n",
      " 'P_exact': 0.0,\n",
      " 'R_exact': 0.0,\n",
      " 'epoch_acc': 0.0}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 85])\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 4->5\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 5->6\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 6->7\n",
      "\n",
      "After dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 7->8\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 8->9\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 9->10\n",
      "\n",
      "After dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=3\n",
      "\tllll_word_score[0].shape=torch.Size([4, 64, 6])\n",
      "Inside Model.training_step method, batch_idx=0 {'train_loss': 2.1155, 'loss': 2.1155}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 85])\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 4->5\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 5->6\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 6->7\n",
      "\n",
      "After dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 7->8\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 8->9\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 9->10\n",
      "\n",
      "After dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=3\n",
      "\tllll_word_score[0].shape=torch.Size([4, 64, 6])\n",
      "Inside Model.training_step method, batch_idx=1 {'train_loss': 1.6847, 'loss': 1.6847}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 85])\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 4->5\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 5->6\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 6->7\n",
      "\n",
      "After dropout\n",
      "\tdepth=1\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=1\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 7->8\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 8->9\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "'lll_hidstate' count changed: 9->10\n",
      "\n",
      "After dropout\n",
      "\tdepth=2\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 85, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 64, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "before argmax\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "after argmax\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "before embedding\n",
      "\tll_greedy_ilabel.shape=torch.Size([4, 64])\n",
      "\n",
      "after embedding\n",
      "\tlll_word_hidstate.state=torch.Size([4, 64, 768])\n",
      "'lll_word_hidstate' count changed: 1->2\n",
      "\n",
      "just summed two signals with this shape\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 64, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=2\n",
      "\tlll_word_score.shape=torch.Size([4, 64, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=3\n",
      "\tllll_word_score[0].shape=torch.Size([4, 64, 6])\n",
      "Inside Model.training_step method, batch_idx=2 {'train_loss': 1.5247, 'loss': 1.5247}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                             | 0/? [00:00<?, ?it/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 115])\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=1\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.validation_step method, batch_idx=0 {'tune_loss': 0.0}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 115])\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=1\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.validation_step method, batch_idx=1 {'tune_loss': 0.0}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 115])\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 115, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=1\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.validation_step method, batch_idx=2 {'tune_loss': 0.0}\n",
      "Entering Model.on_validation_epoch_end method\n",
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n",
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 9: 'tune_epoch_acc' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n",
      "Entering CCMetric.get_score_d method.\n",
      "\n",
      "Scores at end of epoch 2:\n",
      "{'F1_exact': 0.0,\n",
      " 'F1_inner': 0.0,\n",
      " 'F1_outer': 0.0,\n",
      " 'F1_whole': 0.0,\n",
      " 'P_exact': 0.0,\n",
      " 'R_exact': 0.0,\n",
      " 'epoch_acc': 0.0}\n",
      "Saving self.hparams=  \"pi_test\": 3.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at weights_warmup/cc_model/epoch=00_tune_epoch_acc=0.0000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model init\n",
      "\tname=test, hidden_size=768\n",
      "num_iterative_layers=  2\n",
      "num_encoder_layers=  10\n",
      "total num layers=  12\n",
      "CCMetric deleting previous pkl files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from the checkpoint at weights_warmup/cc_model/epoch=00_tune_epoch_acc=0.0000.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38104b10c7ad455a830b67ec44424b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                | 0/? [00:00<?, ?it/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 106])\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=1\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.test_step method, batch_idx=0 {'test_loss': 0.0}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 106])\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=1\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.test_step method, batch_idx=1 {'test_loss': 0.0}\n",
      "Entering model.get_llll_word_score()\n",
      "'lll_hidstate' count changed: 0->1\n",
      "\n",
      "after base_model\n",
      "\tll_osent_icode.shape=torch.Size([4, 106])\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=0\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "'lll_hidstate' count changed: 1->2\n",
      "\n",
      "After iterative layer\n",
      "\tilay=0\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "*********** Starting iterative layer\n",
      "\tilay=1\n",
      "\n",
      "Before iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "'lll_hidstate' count changed: 2->3\n",
      "\n",
      "After iterative layer\n",
      "\tilay=1\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "Before dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "'lll_hidstate' count changed: 3->4\n",
      "\n",
      "After dropout\n",
      "\tdepth=0\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\n",
      "Gather's 2 inputs, then output\n",
      "\tlll_hidstate.shape=torch.Size([4, 106, 768])\n",
      "\tlll_loc.shape=torch.Size([4, 68, 768])\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "'lll_word_hidstate' count changed: 0->1\n",
      "\n",
      "Before merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 768])\n",
      "\n",
      "After merge layer\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "Before ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_hidstate.shape=torch.Size([4, 68, 300])\n",
      "\n",
      "After ilabelling\n",
      "\tdepth=0\n",
      "\tlll_word_score.shape=torch.Size([4, 68, 6])\n",
      "\n",
      "Leaving Model.sax_get_llll_word_score()\n",
      "\tlen(llll_word_score)=1\n",
      "\tllll_word_score[0].shape=torch.Size([4, 68, 6])\n",
      "Inside Model.test_step method, batch_idx=2 {'test_loss': 0.0}\n",
      "Entering Model.on_test_epoch_end method\n",
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n",
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n",
      "Entering CCMetric.__call__() method.\n",
      "number of samples= 4\n",
      "Entering CCMetric.get_score_d method.\n",
      "\n",
      "Scores at end of epoch 0:\n",
      "{'F1_exact': 0.0,\n",
      " 'F1_inner': 0.0,\n",
      " 'F1_outer': 0.0,\n",
      " 'F1_whole': 0.0,\n",
      " 'P_exact': 0.0,\n",
      " 'R_exact': 0.0,\n",
      " 'epoch_acc': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_epoch_acc       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_epoch_acc      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints: ['weights_warmup/cc_model/epoch=00_tune_epoch_acc=0.0000.ckpt']\n"
     ]
    }
   ],
   "source": [
    "main(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "059eb2fbf4b9445ab66ccc2bf28f10d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "05c1b31bf3bc47db84485db27fd8877f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1734c5df1bf14d6584ee5b83eb6406a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "190f263c20f04d908228c9f6f2e3e4a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_81250017084f4563bd9333e7abb87b84",
       "style": "IPY_MODEL_f3e457c62cb149a4b3f6fccbb8b586f9",
       "value": "Validation DataLoader 0: 100%"
      }
     },
     "1c0f4ce1221e48a994c69396936d4e0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1fec138f81b8488d89914a7624d10731": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_059eb2fbf4b9445ab66ccc2bf28f10d6",
       "max": 3,
       "style": "IPY_MODEL_9cd64c17fd3a4f21a44023cfba57c362",
       "value": 3
      }
     },
     "24e6f2b68e3646509ef46cab17ffd35b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_97a48ffb70bb424793412feed28ee7cc",
       "max": 3,
       "style": "IPY_MODEL_b26a384cfd344245aac003fc73dba26a",
       "value": 3
      }
     },
     "29ac87f62eb44149b22cf4e368aa2596": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2ec8d8e51779450d88dbc6c2c14d6472": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "31ba4e70410b45faa45d24c90932d58c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "33714c06e0ad41c0a57c790052617795": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "39bea2186db2465dafd6d2730a8a6c43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_6e8f07fb2a12457783d8d0eb8269d63a",
       "max": 3,
       "style": "IPY_MODEL_6112af9d4f1f4330889ade452d37f5e2",
       "value": 3
      }
     },
     "3aa110d710744c438fd8096fbcc10ba4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_dea7150532484fa581096f6cf4e66b2b",
       "max": 3,
       "style": "IPY_MODEL_f2d250fbe66447dbbb795a2f12ce365c",
       "value": 3
      }
     },
     "3da3e526c57d4b00ad36976f05106a40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "59ccc605b2af46cf97094743b30caf76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5d7aaef590924e6e90d4ef14505a194a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "608e0518dbd746af8f7eabb6cfd3a3e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6112af9d4f1f4330889ade452d37f5e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6e8f07fb2a12457783d8d0eb8269d63a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "71da995dcecb46df97422a2e4284db45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7318ad3af0cd4aa6869af2b1f9db3649": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a1931a8857ea4096aa7171c78c170c74",
        "IPY_MODEL_3aa110d710744c438fd8096fbcc10ba4",
        "IPY_MODEL_846d303852a34c1384ba722a129fa020"
       ],
       "layout": "IPY_MODEL_3da3e526c57d4b00ad36976f05106a40"
      }
     },
     "7a24b40ef9dd4574a20f550d07a39424": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2ec8d8e51779450d88dbc6c2c14d6472",
       "style": "IPY_MODEL_71da995dcecb46df97422a2e4284db45",
       "value": "Testing DataLoader 0: 100%"
      }
     },
     "811fe51ccb094ba898e9a587666f65d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_bbc5f9fe721f4d53aa9286927f05d9e7",
       "style": "IPY_MODEL_a16a4f504b36437f9c6aaae9f3313f76",
       "value": " 3/3 [00:02&lt;00:00,  1.16it/s]"
      }
     },
     "81250017084f4563bd9333e7abb87b84": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "846d303852a34c1384ba722a129fa020": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_916467d9039843b5a466dc7917edc664",
       "style": "IPY_MODEL_29ac87f62eb44149b22cf4e368aa2596",
       "value": " 3/3 [00:06&lt;00:00,  0.49it/s, v_num=part, loss=5.040, loss_step=0.000, loss_epoch=0.000, epoch_acc=0.235]"
      }
     },
     "89466b8358814bc5a31ea83d05e0436e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "89753968b9474a17a0f7045623621476": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_89466b8358814bc5a31ea83d05e0436e",
       "style": "IPY_MODEL_59ccc605b2af46cf97094743b30caf76",
       "value": "Validation DataLoader 0: 100%"
      }
     },
     "8dbbc9284d3143b79dae56b0654d9773": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "90b80f40a523443b96db85ee48dac7c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_91285b0cfa4046958c69d453213dff65",
       "style": "IPY_MODEL_1c0f4ce1221e48a994c69396936d4e0b",
       "value": "Validation DataLoader 0: 100%"
      }
     },
     "91285b0cfa4046958c69d453213dff65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "916467d9039843b5a466dc7917edc664": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "97a48ffb70bb424793412feed28ee7cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "992d78b486c443c2906deb456248c016": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "visibility": "hidden",
       "width": "100%"
      }
     },
     "9cd64c17fd3a4f21a44023cfba57c362": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a16a4f504b36437f9c6aaae9f3313f76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a1931a8857ea4096aa7171c78c170c74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_608e0518dbd746af8f7eabb6cfd3a3e9",
       "style": "IPY_MODEL_8dbbc9284d3143b79dae56b0654d9773",
       "value": "Epoch 2: 100%"
      }
     },
     "a37f42e03a494a07a8389c9eeb25c4b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a3bd3505cc6945d7be3daab3adcd9618",
       "style": "IPY_MODEL_05c1b31bf3bc47db84485db27fd8877f",
       "value": " 3/3 [00:02&lt;00:00,  1.18it/s]"
      }
     },
     "a3bd3505cc6945d7be3daab3adcd9618": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ac390280cce44f698d2ca286e0ff8de5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "visibility": "hidden",
       "width": "100%"
      }
     },
     "b26a384cfd344245aac003fc73dba26a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b8833e6877784cb2a521804da4623e51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_df713549d5ec4736b3dad8feba940118",
       "max": 3,
       "style": "IPY_MODEL_c67bda6012dd49d98632ee38c2b67b3d",
       "value": 3
      }
     },
     "baedf4f1ecb047449ad2b1d748c0cdd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7a24b40ef9dd4574a20f550d07a39424",
        "IPY_MODEL_1fec138f81b8488d89914a7624d10731",
        "IPY_MODEL_a37f42e03a494a07a8389c9eeb25c4b7"
       ],
       "layout": "IPY_MODEL_1734c5df1bf14d6584ee5b83eb6406a7"
      }
     },
     "bbc5f9fe721f4d53aa9286927f05d9e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bd467e665624431bb8777e98baa22956": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "visibility": "hidden",
       "width": "100%"
      }
     },
     "c0ca47b2d45a48938463d298d3126dfc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5d7aaef590924e6e90d4ef14505a194a",
       "style": "IPY_MODEL_33714c06e0ad41c0a57c790052617795",
       "value": " 3/3 [00:02&lt;00:00,  1.15it/s]"
      }
     },
     "c1845c6b0e1e408f82476e17eab60318": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c67bda6012dd49d98632ee38c2b67b3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d6159ae25e8d4c6f8bb4b6ef07ec5919": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_31ba4e70410b45faa45d24c90932d58c",
       "style": "IPY_MODEL_c1845c6b0e1e408f82476e17eab60318",
       "value": " 3/3 [00:02&lt;00:00,  1.05it/s]"
      }
     },
     "dea7150532484fa581096f6cf4e66b2b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "df713549d5ec4736b3dad8feba940118": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "f2d250fbe66447dbbb795a2f12ce365c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f3e457c62cb149a4b3f6fccbb8b586f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
