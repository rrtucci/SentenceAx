# parameters to set of possible values:
# {
#     "accumulate_grad_batches": (2,)
#     "batch_size": (24, 32, 16,)
#     "checkpoint": ("models/warmup_oie_model/epoch=13_eval_acc=0.544.ckpt",)
#     "conj_model": ("models/conj_model/epoch=28_eval_acc=0.854.ckpt",)
#     "constraints": ("posm_hvc_hvr_hve",)
#     "cweights": ("3_3_3_3",)
#     "epochs": (30, 16, 40,)
#     "gpus": (1,)
#     "gradient_clip_val": (1,)
#     "inp": ("carb/data/carb_sentences.txt", "sentences.txt",)
#     "iterative_layers": (2,)
#     "lr": (2E-5, 5e-06,)
#     "mode": ("predict", "train_test", "splitpredict", "resume", "test",)
#     "model_str": ("bert-large-cased", "bert-base-cased",)
#     "multi_opt": (True,)
#     "num_extractions": (5,)
#     "oie_model": ("models/oie_model/epoch=14_eval_acc=0.551_v0.ckpt",)
#     "optimizer": ("adamW", "adam",)
#     "out": ("predictions.txt", "models/results/final",)
#     "rescore_model": ("models/rescore_model",)
#     "rescoring": (True,)
#     "save": (
#     "models/warmup_oie_model", "models/oie_model", "models/conj_model",)
#     "save_k": (3,)
#     "task": ("conj", "oie",)
#     "val_check_interval": (0.1,)
#     "wreg": (1,)
# }

# change dictionary values to custom ones
# define TASK = "custom1", "custom2", etc.
PARAMS_D_LONG = {
    "accumulate_grad_batches": None,
    "batch_size": None,
    "bos_token_id": BOS_TOKEN_ID,  # bos= begin of sentence
    "build_cache": None,
    "checkpoint": None,
    "conj_model": None,
    "constraints": None,  # string like "posm_hvc_hvr_hve"
    "cweights": None,  # constraint weights
    "debug": None,
    "dev_fp": None,
    "dropout": None,
    "eos_token_id": EOS_TOKEN_ID,  # eos= end of sentence
    "epochs": None,
    "gpus": None,
    "gradient_clip_val": None,
    "inp": None,
    "iterative_layers": 2,  # number of transformer layers
    "labelling_dim": None,
    "lr": None,  # lr = learning rate
    "max_steps": None,
    "mode": None,
    "model_str": None,  # model string for base (bert) model
    "multi_opt": None,  # multiple optimizers
    "no_lt": None,  # no local time
    "num_extractions": None,
    "num_sanity_val_steps": None,
    "num_tpu_cores": None,
    "oie_model": None,
    "optimizer": None,
    # "out": None,
    "predict_fp": None,
    "rescore_model": None,
    "rescoring": None,
    # "save": None,
    "save_k": None,
    "split_fp": None,
    "task": None,
    "test_fp": None,
    "track_grad_norm": None,
    "train_fp": None,
    "train_percent_check": None,
    "type": None,
    "use_tpu": None,
    "val_check_interval": None,
    "wreg": None,
    "write_allennlp": None,
    "write_async": None
}
# these params appear in Openie6 but not in its README
additional_dict = {
    "build_cache": None,
    "dev_fp": None,
    "predict_fp": None,
    "split_fp": None,
    "test_fp": None,
    "train_fp": None,
    "type": None,
    "write_allennlp": None
}
# "type" in ["labels", "sentences"] in Openie6. Refers to
# file type, for file to be written too


# combine dictionaries
PARAMS_D = {**PARAMS_D_LONG, **additional_dict}
# eliminate params that are None, in case they are reset to
# default values by pytorch lightning.
PARAMS_D = {key: value for key, value in PARAMS_D_LONG.items()
            if value is not None}
PARAMS_D = none_dd(PARAMS_D)

